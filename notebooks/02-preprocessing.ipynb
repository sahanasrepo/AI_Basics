{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e22b9b",
   "metadata": {},
   "source": [
    "# 02 â€” Text Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da18132",
   "metadata": {},
   "source": [
    "This notebook mirrors **notes/02-text-preprocessing.md** and builds a small pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk scikit-learn\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "# nltk.download(\"punkt\"); nltk.download(\"stopwords\"); nltk.download(\"wordnet\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac5541",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Hey, buddy, I want to go to your house?\",\n",
    "    \"WIN big $$$ now!!! Limited-time offer.\",\n",
    "    \"Reminder: Your meeting is at 3:30pm.\",\n",
    "    \"I do not like this product at all.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc955eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" <URL> \", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \" <EMAIL> \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c6f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words(\"english\")); negators={\"not\",\"no\",\"never\",\"n't\"}\n",
    "porter=PorterStemmer(); lemma=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tokens(text, remove_stopwords=True, stem=False, lemmatize=False):\n",
    "    t = normalize(text)\n",
    "    toks = word_tokenize(t)\n",
    "    out=[]\n",
    "    for tok in toks:\n",
    "        if remove_stopwords and tok in stop and tok not in negators: continue\n",
    "        if stem: tok = porter.stem(tok)\n",
    "        if lemmatize: tok = lemma.lemmatize(tok)\n",
    "        out.append(tok)\n",
    "    return out\n",
    "\n",
    "for s in corpus:\n",
    "    print(s, \"->\", preprocess_tokens(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a214d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = [\" \".join(preprocess_tokens(s)) for s in corpus]\n",
    "cv = CountVectorizer(ngram_range=(1,2)); X = cv.fit_transform(joined); X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40175423",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(ngram_range=(1,2)); Xtf = tf.fit_transform(joined); Xtf.shape"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
