# AI Engineering Notes
> A structured, beginner-to-intermediate reference for IT engineers learning AI/ML/DL.  
> Built from hand-written course notes, expanded with theory, diagrams, and code examples in **both PyTorch and TensorFlow/Keras**.

---

## ğŸ“˜ Learning Path (Read in This Order)

| # | File | What You'll Learn |
|---|------|-------------------|
| 1 | [`01_Foundations.md`](01_Foundations.md) | ML landscape, supervised vs unsupervised, **Bayes' Theorem** (full worked example), linear regression, cost functions |
| 2 | [`02_Deep_Learning.md`](02_Deep_Learning.md) | Perceptrons, activation functions, forward prop, **loss functions**, **gradient descent**, **backpropagation**, vanishing gradient, all major **optimisers** (SGD â†’ Adam), feature scaling |
| 3 | [`03_NLP.md`](03_NLP.md) | NLP pipeline, tokenisation, stopwords, stemming vs lemmatisation, **One-Hot Encoding**, **Bag of Words**, **TF-IDF**, similarity metrics (cosine, Euclidean), N-grams |
| 4 | [`04_Word_Embeddings.md`](04_Word_Embeddings.md) | Why OHE fails at scale, **Word2Vec**, **CBOW** (full worked example with math), **Skip-gram**, embedding layers |
| 5 | [`05_CNN.md`](05_CNN.md) | Image representation, **convolution operation** (edge filters), stride, padding, **max pooling**, flattening, full CNN pipeline, dropout, black/white box models |
| 6 | [`06_RNN_LSTM.md`](06_RNN_LSTM.md) | RNN architecture types, why RNNs forget, **LSTM cell state**, **forget/input/output gates** (all equations + worked example), statistical distributions |
| 7 | [`07_Interview_and_Quick_Reference.md`](07_Interview_and_Quick_Reference.md) | Interview Q&A, DL roadmap (RNNâ†’LSTMâ†’Transformersâ†’BERT), cheat sheets |

---

## ğŸ—ºï¸ The Big Picture â€” Where AI/ML/DL Sit

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          AI                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚              Machine Learning                    â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚   â”‚   â”‚         Deep Learning                    â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   Neural Networks, CNNs, RNNs,           â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   LSTMs, Transformers, BERT, GPT         â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚   â”‚   Also: Random Forest, XGBoost, SVM, KNN         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚   Data Science: Stats + Programming + Domain Knowledge   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š All 50 Images â€” What's Where

```
â”€â”€ NLP & Vectorisation (Images 1â€“11) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Image 01 â”€â”€â–º AI/ML/DL roadmap, tech stack           â”€â”€â–º README + 01_Foundations
Image 02 â”€â”€â–º NLP use-case, preprocessing flow       â”€â”€â–º 03_NLP
Image 03 â”€â”€â–º Stemming vs Lemmatisation              â”€â”€â–º 03_NLP
Image 04 â”€â”€â–º One-Hot Encoding worked example        â”€â”€â–º 03_NLP
Image 05 â”€â”€â–º Cosine similarity, Euclidean distance  â”€â”€â–º 03_NLP
Image 06 â”€â”€â–º N-grams, TF-IDF math                   â”€â”€â–º 03_NLP
Image 07 â”€â”€â–º TF-IDF continued, Word2Vec intro       â”€â”€â–º 03_NLP + 04_Word_Embeddings
Image 08 â”€â”€â–º Word2Vec neural network embedding      â”€â”€â–º 04_Word_Embeddings
Image 09 â”€â”€â–º Embedding layer (one-hot â†’ dense)      â”€â”€â–º 04_Word_Embeddings
Image 10 â”€â”€â–º Word2Vec CBOW architecture             â”€â”€â–º 04_Word_Embeddings
Image 11 â”€â”€â–º Word2Vec Skip-gram                     â”€â”€â–º 04_Word_Embeddings

â”€â”€ Neural Network Fundamentals (Images 12â€“20) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Image 12 â”€â”€â–º Spam classifier (perceptron)           â”€â”€â–º 02_Deep_Learning
Image 13 â”€â”€â–º Perceptron fundamentals                â”€â”€â–º 02_Deep_Learning
Image 14 â”€â”€â–º Forward & backward propagation         â”€â”€â–º 02_Deep_Learning
Image 15 â”€â”€â–º Activation functions (all 5)           â”€â”€â–º 02_Deep_Learning
Image 16 â”€â”€â–º Bayes' Theorem (librarian/farmer)      â”€â”€â–º 01_Foundations
Image 17 â”€â”€â–º Neural net image recognition intro     â”€â”€â–º 02_Deep_Learning
Image 18 â”€â”€â–º Gradient descent mechanics             â”€â”€â–º 02_Deep_Learning
Image 19 â”€â”€â–º Backprop chain rule                    â”€â”€â–º 02_Deep_Learning
Image 20 â”€â”€â–º Loss functions (MSE/MAE/Huber/BCE/CCE) â”€â”€â–º 02_Deep_Learning

â”€â”€ Optimisers & Scaling (Images 21â€“25) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Image 21 â”€â”€â–º Vanishing gradient problem             â”€â”€â–º 02_Deep_Learning
Image 22 â”€â”€â–º SGD, Mini-Batch SGD                    â”€â”€â–º 02_Deep_Learning
Image 23 â”€â”€â–º SGD + Momentum, EWA                    â”€â”€â–º 02_Deep_Learning
Image 24 â”€â”€â–º Adagrad, Adadelta, RMSProp             â”€â”€â–º 02_Deep_Learning
Image 25 â”€â”€â–º Adam optimiser                         â”€â”€â–º 02_Deep_Learning

â”€â”€ CNN (Images 26â€“33) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Image 26 â”€â”€â–º Feature scaling (normalise/standardise)â”€â”€â–º 02_Deep_Learning
Image 27 â”€â”€â–º How images are numbers (grayscale/RGB) â”€â”€â–º 05_CNN
Image 28 â”€â”€â–º Convolution operation, filters         â”€â”€â–º 05_CNN
Image 29 â”€â”€â–º Edge detection filters (horiz/vert)    â”€â”€â–º 05_CNN
Image 30 â”€â”€â–º Stride, padding, output size formula   â”€â”€â–º 05_CNN
Image 31 â”€â”€â–º ReLU on feature maps                   â”€â”€â–º 05_CNN
Image 32 â”€â”€â–º Max pooling worked example             â”€â”€â–º 05_CNN
Image 33 â”€â”€â–º Flattening + full CNN pipeline         â”€â”€â–º 05_CNN

â”€â”€ RNN, LSTM & Wrap-Up (Images 34â€“50) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Image 34 â”€â”€â–º Train/val/test splits, cross-validationâ”€â”€â–º 05_CNN
Image 35 â”€â”€â–º Overfitting & dropout                  â”€â”€â–º 05_CNN
Image 36 â”€â”€â–º Black box vs white box models          â”€â”€â–º 05_CNN
Image 37 â”€â”€â–º Why we need sequence models            â”€â”€â–º 06_RNN_LSTM
Image 38 â”€â”€â–º RNN architecture (the loop)            â”€â”€â–º 06_RNN_LSTM
Image 39 â”€â”€â–º RNN types (1-to-1 â€¦ many-to-many)     â”€â”€â–º 06_RNN_LSTM
Image 40 â”€â”€â–º RNN forward prop worked example        â”€â”€â–º 06_RNN_LSTM
Image 41 â”€â”€â–º Vanishing gradient in RNNs             â”€â”€â–º 06_RNN_LSTM
Image 42 â”€â”€â–º LSTM â€” why it was invented             â”€â”€â–º 06_RNN_LSTM
Image 43 â”€â”€â–º LSTM cell state (conveyor belt)        â”€â”€â–º 06_RNN_LSTM
Image 44 â”€â”€â–º Forget gate + Input gate               â”€â”€â–º 06_RNN_LSTM
Image 45 â”€â”€â–º Cell state update equation             â”€â”€â–º 06_RNN_LSTM
Image 46 â”€â”€â–º Output gate + full LSTM equations      â”€â”€â–º 06_RNN_LSTM
Image 47 â”€â”€â–º Statistical distributions (Normal/Log-Normal/Pareto)â”€â”€â–º 06_RNN_LSTM
Image 48 â”€â”€â–º Box-Cox transformation                 â”€â”€â–º 07_Interview
Image 49 â”€â”€â–º Interview Q&A (splits, RF, Word2Vec)   â”€â”€â–º 07_Interview
Image 50 â”€â”€â–º DL roadmap (RNNâ†’LSTMâ†’â€¦â†’BERT/GPT)      â”€â”€â–º 07_Interview
```

---

## âš™ï¸ Tech Stack Referenced

| Library | Used For |
|---------|----------|
| **NumPy** | Matrix operations, numerical computation |
| **Pandas** | Data loading, feature engineering |
| **Scikit-learn** | Train/test split, scaling, metrics, classical ML |
| **PyTorch** | Neural network definitions, custom training loops |
| **TensorFlow / Keras** | High-level model building, Sequential API |
| **NLTK** | Tokenisation, stopwords, stemming |
| **Gensim** | Pre-trained Word2Vec, embedding similarity |
| **Matplotlib** | Plotting, visualisation |

---

## ğŸ“ Conventions Used Throughout

- **Mathematical formulas** use standard ML notation: `w` = weights, `b` = bias, `Î·` = learning rate, `Ïƒ()` = sigmoid
- **ASCII diagrams** render in any monospace font â€” designed for GitHub markdown
- **Code blocks** come in pairs where relevant: PyTorch version + TensorFlow/Keras version
- **â­** marks the recommended default choice (e.g. â­ Adam optimiser, â­ ReLU activation)

---

*Source: Hand-written notes from AI/ML YouTube courses, expanded and structured for clarity.*
